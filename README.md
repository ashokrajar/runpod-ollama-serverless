# Ollama in RunPod Serverless

This repo containers all the helper code required to run your Ollama Service in `RunPod GPU as a Serverless service`.


